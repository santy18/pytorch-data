{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c87a7e-b781-4168-ad71-6f30b761a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# === HISTORY: Why Transformers? ===\n",
    "# Before transformers, most models for sequences (like text) used RNNs (Recurrent Neural Networks).\n",
    "# RNNs process data one token at a time, which is slow and has trouble with long dependencies.\n",
    "# Transformers changed this by processing all tokens at once and using \"attention\" to decide what matters.\n",
    "\n",
    "# === This class builds a very basic Transformer Encoder model ===\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=32, nhead=4, dim_feedforward=64, max_len=100):\n",
    "        \"\"\"\n",
    "        vocab_size: How many unique words/tokens we have.\n",
    "        d_model: Size of each word/token embedding vector.\n",
    "        nhead: Number of attention heads (part of multi-head attention).\n",
    "        dim_feedforward: Size of internal FFN layer.\n",
    "        max_len: Max number of tokens in a sequence.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Step 1: Turn each token into a vector using an embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Step 2: Add positional info (so model knows the order of words)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Step 3: Define a basic Transformer Encoder Layer (1 layer only)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,        # input & output vector size\n",
    "            nhead=nhead,            # how many attention \"heads\" to use\n",
    "            dim_feedforward=dim_feedforward  # internal hidden size\n",
    "        )\n",
    "\n",
    "        # Step 4: Wrap that one layer into a full encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "\n",
    "        # Step 5: Project output to vocab size (like predicting next word)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, seq_len), containing token IDs\n",
    "        \"\"\"\n",
    "        # Embed the tokens (from IDs to vectors)\n",
    "        embedded = self.embedding(x)  # shape: (batch, seq_len, d_model)\n",
    "\n",
    "        # Add position information\n",
    "        embedded = self.positional_encoding(embedded)\n",
    "\n",
    "        # PyTorch Transformer wants (seq_len, batch, d_model)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        # Run through the Transformer Encoder\n",
    "        encoded = self.transformer_encoder(embedded)\n",
    "\n",
    "        # Convert back to (batch, seq_len, d_model)\n",
    "        encoded = encoded.permute(1, 0, 2)\n",
    "\n",
    "        # Final prediction layer (e.g. for classification or next-word prediction)\n",
    "        return self.fc_out(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c2034-69f1-4a75-99f7-bf7b6778eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        PositionalEncoding gives the model info about the position of each word.\n",
    "        This is critical because the Transformer has no idea of order by default.\n",
    "        We use sine & cosine functions (from original 2017 paper) to generate these.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Positions from 0 to max_len-1, reshaped to (max_len, 1)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        # Exponential decay terms for frequency\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Even positions = sine, Odd positions = cosine\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension so we can add it to embeddings later\n",
    "        pe = pe.unsqueeze(0)  # shape becomes (1, max_len, d_model)\n",
    "\n",
    "        # Register buffer so it's saved with the model but not trainable\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input embeddings\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796e6e3-b741-439a-a1ae-1cbea699424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocab size (like 100 tokens/words total)\n",
    "vocab_size = 100\n",
    "\n",
    "# Create model\n",
    "model = SimpleTransformer(vocab_size)\n",
    "\n",
    "# Sample batch: 2 sequences, each 10 tokens long\n",
    "x = torch.randint(0, vocab_size, (2, 10))  # shape = (batch=2, seq_len=10)\n",
    "\n",
    "# Get model output\n",
    "output = model(x)  # shape = (2, 10, vocab_size)\n",
    "\n",
    "print(output.shape)  # Expect: [2, 10, 100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ba0f1-eaf1-4829-b40f-008c5cb7996e",
   "metadata": {},
   "source": [
    "## ðŸ§  Summary (Why Each Part Exists)\n",
    "\n",
    "| Component              | Purpose                                         |\n",
    "|------------------------|-------------------------------------------------|\n",
    "| `Embedding`            | Turns token IDs into dense vectors              |\n",
    "| `PositionalEncoding`   | Injects info about word order                   |\n",
    "| `TransformerEncoderLayer` | Applies attention + feedforward logic       |\n",
    "| `TransformerEncoder`   | Stack of encoder layers                         |\n",
    "| `Linear`               | Projects back to token space (e.g., to predict next word) |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… You can train this model on:\n",
    "- Next-token prediction tasks\n",
    "- Simple classification problems (e.g., sentiment)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Let me know if you want:\n",
    "- A training loop  \n",
    "- A dataset example (like text)  \n",
    "- Visualization of attention heads  \n",
    "- Extension to encoder-decoder (like T5 or GPT)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
